# -*- coding: utf-8 -*-
"""MNIST_train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Mu5eJH90IvszSkq_FVBAkP8ro8REHMUK

#MNIST-Handwritten_digit_classification

**This is a multiclass(more specifically 10 class) classifiaction problem.**

The MNIST database of handwritten digits, available from this [page](http://yann.lecun.com/exdb/mnist/), has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST.You can use the above dataset or use it from keras dataset(without downloading !).I have used the latter approach.
"""

#Importing the necessary libraries
import numpy as np
import matplotlib.pyplot as plt
from tensorflow import keras
from tensorflow.keras import layers
from IPython.display import clear_output

"""##Preparing the data to be fed into our model



*   We have 10 classes( 0 to 9)
*   This is a grayscale image of 28*28 pixels
"""

number_classes=10
input_shape_of_image=(28,28,1)

"""###Splitting the dataset into train and test and getting its shape !"""

(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

"""#Let's visulise one of the image using matplotlib library"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
image_index = 1234
print(y_train[image_index])
plt.imshow(np.squeeze(x_train[image_index]), cmap='Greys')

"""###Feature Scaling (so that pixel values lie in the range [0,1] for faster training and convergence."""

x_train=x_train.astype("float32")/255
x_test=x_test.astype("float32")/255

#checking the shape of images and number of samples of training and testing data
x_train=np.expand_dims(x_train,-1)
x_test=np.expand_dims(x_test,-1)

print("X_train_shape ::\n",x_train.shape)
print("\nX_test shape ::\n",x_test.shape)
print("\n")
print(x_train.shape[0]," training_samples\n")
print(x_test.shape[0]," testing samples\n")

"""###Convert output vector to binary class metrics"""

y_train=keras.utils.to_categorical(y_train,number_classes)
y_test=keras.utils.to_categorical(y_test,number_classes)

"""#Let's visulise one of the image using matplotlib library

We have set up our data

Now its time to build our model.We'll try buliding three different models.

##Model



1.   M1-3Conv layer with max pooling
2.   M2-5 conv layer with dropout
3.   M3-7 conv layer with Batch Normalisation
"""

class PlotLosses(keras.callbacks.Callback):
    def on_train_begin(self, logs={}):
        self.i = 0
        self.x = []
        self.losses = []
        self.val_losses = []
        
        self.fig = plt.figure()
        
        self.logs = []

    def on_epoch_end(self, epoch, logs={}):
        
        self.logs.append(logs)
        self.x.append(self.i)
        self.losses.append(logs.get('loss'))
        self.val_losses.append(logs.get('val_loss'))
        self.i += 1
        
        clear_output(wait=True)
        plt.plot(self.x, self.losses, label="loss")
        plt.plot(self.x, self.val_losses, label="val_loss")
        plt.legend()
        plt.show();
        
plot_losses = PlotLosses()

"""#Model 1"""

#Defining the architecture of M1


model1=keras.Sequential(
    [
     keras.Input(shape=input_shape_of_image),
     layers.Conv2D(32,kernel_size=(3,3),activation="relu"),
     layers.MaxPooling2D(pool_size=(2,2)),
     layers.Conv2D(64,kernel_size=(3,3),activation="relu"),
     layers.MaxPooling2D(pool_size=(2,2)),
     #layers.Conv2D(128,kernel_size=(3,3),activation="relu"),
     #layers.MaxPooling2D(pool_size=(2,2)),
     layers.Flatten(),
     layers.Dense(number_classes,activation="softmax"),
    ]
)

#Checking the summary of the model
model1.summary()

"""#Model 2"""

#Defining the architecture of M2


model2=keras.Sequential(
    [
     keras.Input(shape=input_shape_of_image),
     layers.Conv2D(32,kernel_size=(3,3),activation="relu"),
     layers.MaxPooling2D(pool_size=(2,2)),
     layers.Conv2D(64,kernel_size=(3,3),activation="relu"),
     layers.MaxPooling2D(pool_size=(2,2)),
     #layers.Conv2D(128,kernel_size=(3,3),activation="relu"),
     #layers.MaxPooling2D(pool_size=(2,2)),
     layers.Flatten(),
     layers.Dropout(0.3),
     layers.Dense(number_classes,activation="softmax"),
    ]
)

model2.summary()

#Train the model 2

batch_size=64
epochs=10

model2.compile(loss="categorical_crossentropy",optimizer="adam",metrics=["accuracy"])
model2.fit(x_train,y_train,batch_size=batch_size,epochs=epochs,validation_split=0.10,callbacks=[plot_losses],verbose=0)

#Train the model 1

batch_size=64
epochs=15

model1.compile(loss="categorical_crossentropy",optimizer="adam",metrics=["accuracy"])
model1.fit(x_train,y_train,batch_size=batch_size,epochs=epochs,validation_split=0.10,callbacks=[plot_losses],verbose=0)

score=model1.evaluate(x_test,y_test,verbose=0)
print("Test Loss :\t",score[0])
print("Test Accuracy :\t",score[1])

score=model2.evaluate(x_test,y_test,verbose=0)
print("Test Loss :\t",score[0])
print("Test Accuracy :\t",score[1])

"""We got good results with **model 2**.
Let's go with model 2
"""

def create_model():
    # create model
    model=keras.Sequential(
    [
     keras.Input(shape=input_shape_of_image),
     layers.Conv2D(32,kernel_size=(3,3),activation="relu"),
     layers.MaxPooling2D(pool_size=(2,2)),
     layers.Conv2D(64,kernel_size=(3,3),activation="relu"),
     layers.MaxPooling2D(pool_size=(2,2)),
     #layers.Conv2D(128,kernel_size=(3,3),activation="relu"),
     #layers.MaxPooling2D(pool_size=(2,2)),
     layers.Flatten(),
     layers.Dropout(0.3),
     layers.Dense(number_classes,activation="softmax"),
    ]
)
    return model

# build the model
model = create_model()

# Fit the model

batch_size=64
epochs=10

model.compile(loss="categorical_crossentropy",optimizer="adam",metrics=["accuracy"])
model.fit(x_train,y_train,batch_size=batch_size,epochs=epochs,validation_split=0.10,callbacks=[plot_losses],verbose=0)

model.save("model.h5")
print("model weights saved in model.h5 file")

from keras.models import model_from_json
model_json = model.to_json()
with open("model.json", "w") as json_file:
    json_file.write(model_json)
print("model saved as model.json file")

!ls

from google.colab import files
files.download("model.json")

from google.colab import files
files.download("model.h5")

